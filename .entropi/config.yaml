# Entropi Global Configuration

models:
  thinking:
    path: ~/models/gguf/Qwen_Qwen3-14B-Q4_K_M.gguf
    adapter: qwen3
    context_length: 16384  # Reduced for VRAM (32768 OOMs)
    gpu_layers: -1
  normal:
    path: ~/models/gguf/Falcon-H1R-7B-Q8_0.gguf
    adapter: falcon
    context_length: 32768
    gpu_layers: -1
  code:
    path: ~/models/gguf/Falcon-H1R-7B-Q8_0.gguf
    adapter: falcon
    context_length: 32768
    gpu_layers: -1
  simple:
    # Simple responses - uses same model as normal
    path: ~/models/gguf/Falcon-H1R-7B-Q8_0.gguf
    adapter: falcon
    context_length: 32768
    gpu_layers: -1
  router:
    # Classification only - small model for task routing
    path: ~/models/gguf/qwen2.5-0.5b-instruct-q8_0.gguf
    adapter: qwen2
    context_length: 4096
    gpu_layers: -1
  default: normal

thinking:
  enabled: false

routing:
  enabled: true
  fallback_model: normal

generation:
  max_tokens: 4096  # Per-response token limit
  default_temperature: 0.7
  default_top_p: 0.9

log_level: DEBUG
